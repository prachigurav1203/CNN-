{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is the role of filters and feature maps in Convolutional Neural\n",
        "Network (CNN)?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "In a Convolutional Neural Network (CNN), filters and feature maps play a central role in enabling the network to automatically learn meaningful patterns from input images. A filter, also called a kernel, is a small matrix of learnable weights that slides over the input image and performs convolution. Each filter is designed to detect a specific type of feature such as edges, textures, corners, color gradients, or more complex structures. During training, the network adjusts the values in these filters so they can identify patterns that are useful for classification or recognition tasks. A CNN typically uses multiple filters in each convolutional layer, allowing it to learn a wide variety of features at different spatial positions.\n",
        "\n",
        "When a filter is applied to an image, it produces a feature map, also known as the activation map or output map. The feature map represents how strongly the filter responds to various regions of the image. For example, if a particular filter is trained to detect vertical edges, its feature map will highlight areas in the image where vertical edges exist. As a CNN progresses through deeper layers, feature maps capture increasingly complex and abstract patterns. Early layers detect simple features like edges or corners, intermediate layers detect shapes or object parts, and deeper layers capture high-level concepts like faces, objects, or textures.\n",
        "\n",
        "The interaction between filters and feature maps allows CNNs to learn spatial hierarchies of features, meaning simple patterns combine to form more complex patterns in deeper layers. This hierarchical learning is what makes CNNs highly effective for tasks such as image classification, object detection, and visual recognition. Furthermore, because the same filter slides across the entire image, CNNs achieve translation invariance—meaning they can detect an object regardless of its location in the image. Overall, filters extract meaningful visual patterns, while feature maps record the presence and strength of those patterns, enabling CNNs to understand and learn from image data effectively."
      ],
      "metadata": {
        "id": "YxZOgk3VM5lw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: Explain the concepts of padding and stride in CNNs(Convolutional Neural\n",
        "Network). How do they affect the output dimensions of feature maps?**\n",
        "\n",
        "Answer:\n",
        "In Convolutional Neural Networks (CNNs), padding and stride are two important concepts that directly influence how the convolution operation processes an image and how the size of the output feature map is determined.\n",
        "\n",
        "Padding refers to the practice of adding extra rows and columns (usually filled with zeros) around the edges of the input image before applying a convolution filter. Padding is used for two main reasons. First, it helps preserve spatial information at the borders of the image. Without padding, the filter cannot fully cover the boundary pixels, causing the output feature map to shrink with every convolution layer. Second, padding allows the network to control the size of the output feature map. With “same” padding, enough zeros are added so that the output feature map has the same spatial dimensions as the input. With “valid” padding, no padding is added, which reduces the dimensions after convolution. Padding therefore helps maintain resolution, retain edge information, and prevent excessive shrinking of feature maps in deep CNNs.\n",
        "\n",
        "Stride, on the other hand, controls how the filter moves across the input image. A stride of 1 means the filter shifts one pixel at a time, generating dense and high-resolution feature maps. If the stride is increased to 2 or more, the filter jumps over pixels, resulting in a smaller output feature map and a reduction in computational cost. Larger strides help down-sample the input, similar to pooling, but they may also cause a loss of detailed spatial information because fewer regions of the input are covered.\n",
        "\n",
        "Both padding and stride significantly affect the output dimensions of feature maps. When padding is applied, the output size increases or remains the same depending on the type of padding. Without padding, the output size becomes smaller. Stride also influences the output size: the larger the stride, the smaller the feature map becomes because fewer positions are evaluated. Mathematically, the output dimension can be computed using the formula:\n",
        "\n",
        "Output Size\n",
        "=S(N+2P−F)​+1\n",
        "\n",
        "where N is the input size, P is the padding, F is the filter size, and S is the stride. Thus, padding helps keep the dimensions larger, while stride controls how much the feature map is reduced. Together, they shape the resolution and amount of information extracted in each convolutional layer of a CNN.\n"
      ],
      "metadata": {
        "id": "56oM5quIM5iO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: Define receptive field in the context of CNNs. Why is it important for deep\n",
        "architectures?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "In the context of Convolutional Neural Networks (CNNs), the receptive field refers to the specific region of the input image that influences the value of a particular neuron in a feature map. In other words, it is the area of the input that a neuron “looks at” or is sensitive to when computing its activation. In early layers of a CNN, the receptive field is small because filters are small (e.g., 3×3 or 5×5), so a neuron reacts only to local features such as edges or corners. As the network becomes deeper, with multiple convolution and pooling layers stacked on top of each other, the receptive field of neurons in deeper layers grows larger. This happens because each neuron in a deeper layer receives input from a region of the feature map that itself corresponds to a region of the original image. Thus, deeper neurons effectively learn higher-level and more abstract representations of the input.\n",
        "\n",
        "The concept of receptive field is extremely important for deep architectures because it defines how much “context” the network can capture. For tasks like image classification, object detection, or semantic segmentation, understanding large regions or the entire object is essential. If the receptive field is too small, the network may only learn fine-grained local details and fail to recognize global structures. In contrast, a sufficiently large receptive field allows deeper layers to integrate information from large portions of the image, enabling the recognition of shapes, patterns, and object parts. This is especially important in modern deep CNNs, where multiple layers work together to extract hierarchical features—from low-level edges in early layers to full object representations in deeper layers.\n",
        "\n",
        "Moreover, an appropriately large receptive field helps a deep CNN cope with challenges such as variations in scale, object deformation, and noise. It ensures that the model does not rely solely on local pixel patterns but can understand the overall spatial arrangement of features. Techniques like using larger filter sizes, stacking more convolution layers, using dilated convolutions, and applying pooling layers all contribute to expanding the receptive field. In summary, the receptive field is a fundamental concept that determines how much of the input image each neuron can interpret, and it is crucial for enabling deep CNN architectures to learn meaningful, high-level representations required for complex vision tasks.\n"
      ],
      "metadata": {
        "id": "h4ygPJW2M5ah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: Discuss how filter size and stride influence the number of parameters in a\n",
        "CNN.**\n",
        "\n",
        "Answer:\n",
        "In a Convolutional Neural Network (CNN), the filter size and stride are two architectural choices that significantly influence how the network processes data, and they indirectly affect the number of parameters and computational complexity. The filter size (or kernel size) refers to the height and width of the filter matrix, such as 3×3, 5×5, or 7×7. The number of learnable parameters in a convolutional layer is determined by the formula:\n",
        "\n",
        "Parameters\n",
        "\n",
        "Parameters=(Fh​×Fw​×Cin​)×Cout​+Cout​\n",
        "\n",
        " is the number of filters (i.e., output channels). From this formula, it is clear that the filter size directly influences the number of parameters: larger filters have more weights and thus increase the total number of parameters. For example, a 5×5 filter has 25 weights per channel, whereas a 3×3 filter has only 9. This is why modern CNN architectures typically use small filters like 3×3 to reduce parameters while maintaining receptive field growth through stacked layers.\n",
        "\n",
        "On the other hand, stride determines how far the filter moves across the input image during convolution. While stride does not change the actual number of learnable parameters—because parameter count depends only on filter size, number of channels, and number of filters—it does influence the total number of computations and the size of the output feature map. A larger stride reduces the spatial dimensions of the output feature map, which decreases the number of activations that need to be computed, thus reducing computational cost. A stride of 1 produces a dense and large output map, requiring more computations, whereas a stride of 2 or more produces smaller output maps, reducing computational burden.\n",
        "\n",
        "Therefore, while stride does not directly change the number of weights in the model, it has an indirect influence on the effective complexity of the model by affecting how many times the filters are applied to the input. Larger strides reduce the number of applications of the filter and consequently reduce memory usage and processing time. In contrast, filter size directly increases or decreases the number of parameters the network must learn. Together, filter size and stride shape the efficiency, depth behavior, and computational requirements of a CNN, making them essential considerations when designing convolutional architectures."
      ],
      "metadata": {
        "id": "3rCjI9hVM5Mr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: Compare and contrast different CNN-based architectures like LeNet,\n",
        "AlexNet, and VGG in terms of depth, filter sizes, and performance.**\n",
        "\n",
        "Answer:\n",
        "\n",
        "CNN-based architectures such as LeNet, AlexNet, and VGG represent important milestones in the evolution of deep learning for computer vision. They differ significantly in terms of depth, filter sizes, design philosophy, and performance, reflecting how CNNs advanced over time from simple networks to very deep architectures capable of high accuracy on large-scale datasets.\n",
        "\n",
        "LeNet-5, introduced by Yann LeCun in 1998, is one of the earliest CNN architectures and was primarily designed for handwritten digit recognition (MNIST dataset). It is a relatively shallow network with around 5–7 layers, including convolutional, pooling, and fully connected layers. LeNet uses small filter sizes such as 5×5 and employs tanh as its activation function. Its depth and complexity are modest because it was developed at a time when computational resources were extremely limited. Despite being simple, LeNet established the fundamental building blocks of modern CNNs.\n",
        "\n",
        "AlexNet, introduced by Alex Krizhevsky in 2012, marked a major breakthrough in deep learning by winning the ImageNet competition with a massive performance improvement over existing methods. AlexNet is much deeper than LeNet, with 8 layers (5 convolutional and 3 fully connected layers) and millions of parameters. It uses larger filter sizes like 11×11 and 5×5 in early layers, though later layers use smaller 3×3 kernels. AlexNet introduced several innovations, such as the ReLU activation function for faster training, dropout for regularization, and data augmentation to reduce overfitting. It also used GPU acceleration, which made training practical for the first time on large datasets. AlexNet demonstrated the power of deep networks on complex, high-resolution images.\n",
        "\n",
        "VGG, developed by Simonyan and Zisserman in 2014, pushed the idea of depth even further. VGG networks, such as VGG16 and VGG19, contain 16 and 19 layers respectively, making them significantly deeper than AlexNet. Unlike AlexNet, VGG uses a very consistent design pattern: it relies exclusively on small 3×3 filters stacked sequentially, often in groups of two or three, followed by max pooling. This design strategy shows that deeper networks with small filters can achieve better performance while keeping computations manageable. VGG networks achieve very high accuracy on ImageNet and are widely used as feature extractors in transfer learning applications. However, they contain a very large number of parameters—up to 138 million—which makes them computationally expensive and memory-intensive.\n",
        "\n",
        "In terms of performance, LeNet works well on small grayscale images but is not suitable for complex, large-scale datasets. AlexNet significantly improved accuracy and demonstrated the potential of deep learning, especially when combined with GPUs and ReLU. VGG further improved performance and is known for its simplicity and uniform architecture, though it is computationally heavy. Together, LeNet, AlexNet, and VGG represent the progression from early CNNs to deep and high-performing models, illustrating how architectural depth, filter design, and computational power collectively contribute to improved recognition accuracy."
      ],
      "metadata": {
        "id": "CabKwEnxOivD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6: Using keras, build and train a simple CNN model on the MNIST dataset\n",
        "from scratch. Include code for module creation, compilation, training, and evaluation.\n",
        "(Include your Python code and output in the code box below.)**\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "9YfvYR61OinA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia8u-a3gMyBd",
        "outputId": "acca98d9-4c12-4f60-ebcf-7104eeacc324"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 50ms/step - accuracy: 0.8761 - loss: 0.4340 - val_accuracy: 0.9833 - val_loss: 0.0577\n",
            "Epoch 2/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 52ms/step - accuracy: 0.9815 - loss: 0.0607 - val_accuracy: 0.9823 - val_loss: 0.0616\n",
            "Epoch 3/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 51ms/step - accuracy: 0.9875 - loss: 0.0399 - val_accuracy: 0.9873 - val_loss: 0.0426\n",
            "Epoch 4/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 49ms/step - accuracy: 0.9906 - loss: 0.0293 - val_accuracy: 0.9887 - val_loss: 0.0363\n",
            "Epoch 5/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 50ms/step - accuracy: 0.9931 - loss: 0.0212 - val_accuracy: 0.9905 - val_loss: 0.0361\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9872 - loss: 0.0409\n",
            "Test accuracy: 0.9890\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1)).astype('float32') / 255.0\n",
        "x_test = x_test.reshape((x_test.shape[0], 28, 28, 1)).astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Build a simple CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Load and preprocess the CIFAR-10 dataset using Keras, and create a\n",
        "CNN model to classify RGB images. Show your preprocessing and architecture.\n",
        "(Include your Python code and output in the code box below.)**\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "ogkBR7pfO6V4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.astype('float32') / 255.0  # Normalize pixel values to [0,1]\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(32,32,3)),\n",
        "    layers.Conv2D(32, (3,3), activation='relu', padding='same'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "    layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zabA_UEZO0Sg",
        "outputId": "a7215563-1618-4291-dfd7-ffea77164dab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 345ms/step - accuracy: 0.3180 - loss: 1.8321 - val_accuracy: 0.5284 - val_loss: 1.3103\n",
            "Epoch 2/5\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 329ms/step - accuracy: 0.5578 - loss: 1.2290 - val_accuracy: 0.6548 - val_loss: 0.9885\n",
            "Epoch 3/5\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 334ms/step - accuracy: 0.6407 - loss: 1.0026 - val_accuracy: 0.6934 - val_loss: 0.8914\n",
            "Epoch 4/5\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 328ms/step - accuracy: 0.6868 - loss: 0.8792 - val_accuracy: 0.7432 - val_loss: 0.7698\n",
            "Epoch 5/5\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 334ms/step - accuracy: 0.7211 - loss: 0.7846 - val_accuracy: 0.7540 - val_loss: 0.7122\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 43ms/step - accuracy: 0.7402 - loss: 0.7493\n",
            "Test accuracy: 0.7391\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Using PyTorch, write a script to define and train a CNN on the MNIST\n",
        "dataset. Include model definition, data loaders, training loop, and accuracy evaluation.\n",
        "(Include your Python code and output in the code box below.)**\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "zxfsLyW6PEfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define transformations for MNIST\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),          # Convert PIL image to tensor\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize mean & std\n",
        "])\n",
        "\n",
        "# Load MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Define CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)  # 1 input channel, 32 filters, 3x3 kernel\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = CNN().to(device)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch}, Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        total += target.size(0)\n",
        "\n",
        "print(f\"Test Accuracy: {correct / total:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3vgOhy2PG63",
        "outputId": "e84424cc-57c1-4c44-c83f-631863f9ed71"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 6.09MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 161kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.52MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.06MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.2238\n",
            "Epoch 2, Loss: 0.0973\n",
            "Epoch 3, Loss: 0.0761\n",
            "Epoch 4, Loss: 0.0638\n",
            "Epoch 5, Loss: 0.0519\n",
            "Test Accuracy: 0.9908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Given a custom image dataset stored in a local directory, write code using\n",
        "Keras ImageDataGenerator to preprocess and train a CNN model.\n",
        "(Include your Python code and output in the code box below.)**\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "32cSnQ4NPNXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Define a simple CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(32,32,3)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Conv2D(128, (3,3), activation='relu', padding='same'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=64,\n",
        "    epochs=5,\n",
        "    validation_split=0.1\n",
        ")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rQG0KfAPWDO",
        "outputId": "fe13fcc0-c3bf-4fce-d729-4c2c9d65c249"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 138ms/step - accuracy: 0.3374 - loss: 1.7935 - val_accuracy: 0.5646 - val_loss: 1.2159\n",
            "Epoch 2/5\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 139ms/step - accuracy: 0.5850 - loss: 1.1650 - val_accuracy: 0.6478 - val_loss: 1.0150\n",
            "Epoch 3/5\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 138ms/step - accuracy: 0.6674 - loss: 0.9505 - val_accuracy: 0.6866 - val_loss: 0.8999\n",
            "Epoch 4/5\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 138ms/step - accuracy: 0.7103 - loss: 0.8291 - val_accuracy: 0.7220 - val_loss: 0.8141\n",
            "Epoch 5/5\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 140ms/step - accuracy: 0.7469 - loss: 0.7263 - val_accuracy: 0.7326 - val_loss: 0.7880\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - accuracy: 0.7238 - loss: 0.8234\n",
            "Test accuracy: 0.7142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: You are working on a web application for a medical imaging startup. Your\n",
        "task is to build and deploy a CNN model that classifies chest X-ray images into “Normal”\n",
        "and “Pneumonia” categories. Describe your end-to-end approach–from data preparation\n",
        "and model training to deploying the model as a web app using Streamlit.\n",
        "(Include your Python code and output in the code box below.)**\n",
        "\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "sfs_tnhxPNQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# Load CIFAR-10 dataset as example (simulate 2-class problem)\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# For demo: Use classes 0 and 1 as Normal and Pneumonia\n",
        "train_mask = np.isin(y_train, [0,1]).flatten()\n",
        "test_mask = np.isin(y_test, [0,1]).flatten()\n",
        "\n",
        "x_train, y_train = x_train[train_mask], y_train[train_mask]\n",
        "x_test, y_test = x_test[test_mask], y_test[test_mask]\n",
        "\n",
        "# Convert labels: 0 -> Normal, 1 -> Pneumonia\n",
        "y_train = (y_train == 1).astype(int)\n",
        "y_test = (y_test == 1).astype(int)\n",
        "\n",
        "# Normalize images\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Build CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Evaluate\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Save model\n",
        "model.save(\"chest_xray_cnn_demo.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-2Wkz-oe55H",
        "outputId": "420d6ad1-fe2d-43d4-bb87-25dc736e9698"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 45ms/step - accuracy: 0.7770 - loss: 0.4556 - val_accuracy: 0.8940 - val_loss: 0.2554\n",
            "Epoch 2/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 42ms/step - accuracy: 0.9008 - loss: 0.2454 - val_accuracy: 0.9290 - val_loss: 0.1881\n",
            "Epoch 3/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - accuracy: 0.9188 - loss: 0.2080 - val_accuracy: 0.9140 - val_loss: 0.2010\n",
            "Epoch 4/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - accuracy: 0.9306 - loss: 0.1733 - val_accuracy: 0.9430 - val_loss: 0.1563\n",
            "Epoch 5/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 45ms/step - accuracy: 0.9449 - loss: 0.1402 - val_accuracy: 0.9450 - val_loss: 0.1360\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9521 - loss: 0.1330\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9460\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model(\"chest_xray_cnn_demo.h5\")\n",
        "\n",
        "st.title(\"Chest X-Ray Pneumonia Detection Demo\")\n",
        "st.write(\"Upload an X-ray image (simulated CIFAR-10 demo)\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\",\"png\",\"jpeg\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    img = image.load_img(uploaded_file, target_size=(32,32))\n",
        "    img_array = image.img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    prediction = model.predict(img_array)[0][0]\n",
        "    if prediction > 0.5:\n",
        "        st.error(f\"Prediction: Pneumonia ({prediction:.2f})\")\n",
        "    else:\n",
        "        st.success(f\"Prediction: Normal ({1-prediction:.2f})\")\n",
        "\n",
        "    st.image(img, caption='Uploaded Image', use_column_width=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTWuUo9ee6bs",
        "outputId": "4543333b-2890-49f3-ef76-4574f5a9795d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "2025-11-29 12:04:12.558 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-29 12:04:12.751 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-11-29 12:04:12.753 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-29 12:04:12.754 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-29 12:04:12.756 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-29 12:04:12.757 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-29 12:04:12.758 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-29 12:04:12.759 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-29 12:04:12.761 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-29 12:04:12.762 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-29 12:04:12.763 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-29 12:04:12.765 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-29 12:04:12.766 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FD9EwARbfVU8",
        "outputId": "8776b03d-7e74-4e7b-83e0-456f87bc6549"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.51.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow<22,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.12.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.11.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.29.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.51.0-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.51.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68FRD42Dffb6",
        "outputId": "f7e49edc-ede1-48cd-e101-a75ee89618ff"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: streamlit run [OPTIONS] [TARGET] [ARGS]...\n",
            "Try 'streamlit run --help' for help.\n",
            "\n",
            "Error: Invalid value: File does not exist: app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "st.title(\"Hello Streamlit!\")\n",
        "st.write(\"This is a test app.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3w8ePaROgEow",
        "outputId": "cac2c8e0-abbb-4f47-a6c6-3dc308fb0a2f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-29 12:07:28.521 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-29 12:07:28.523 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-29 12:07:28.525 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-29 12:07:28.526 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-29 12:07:28.527 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-29 12:07:28.528 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kUchHU4CgsxS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}